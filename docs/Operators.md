## Operator Schemas
*This file is automatically generated from the
            [def files](/onnx/defs) via [this script](/onnx/defs/gen_doc.py).
            Do not modify directly and instead edit operator definitions.*

* <a href="#Abs">Abs</a>
* <a href="#Add">Add</a>
* <a href="#ArgMax">ArgMax</a>
* <a href="#ArgMin">ArgMin</a>
* <a href="#AveragePool">AveragePool</a>
* <a href="#BatchNormalization">BatchNormalization</a>
* <a href="#Cast">Cast</a>
* <a href="#Ceil">Ceil</a>
* <a href="#Concat">Concat</a>
* <a href="#Constant">Constant</a>
* <a href="#Conv">Conv</a>
* <a href="#ConvTranspose">ConvTranspose</a>
* <a href="#Div">Div</a>
* <a href="#Dropout">Dropout</a>
* <a href="#Elu">Elu</a>
* <a href="#Exp">Exp</a>
* <a href="#Flatten">Flatten</a>
* <a href="#Floor">Floor</a>
* <a href="#Gather">Gather</a>
* <a href="#Gemm">Gemm</a>
* <a href="#GlobalAveragePool">GlobalAveragePool</a>
* <a href="#GlobalLpPool">GlobalLpPool</a>
* <a href="#GlobalMaxPool">GlobalMaxPool</a>
* <a href="#LRN">LRN</a>
* <a href="#LeakyRelu">LeakyRelu</a>
* <a href="#Log">Log</a>
* <a href="#LpPool">LpPool</a>
* <a href="#MatMul">MatMul</a>
* <a href="#Max">Max</a>
* <a href="#MaxPool">MaxPool</a>
* <a href="#MaxRoiPool">MaxRoiPool</a>
* <a href="#Min">Min</a>
* <a href="#Mul">Mul</a>
* <a href="#Neg">Neg</a>
* <a href="#OptimizedRNN">OptimizedRNN</a>
* <a href="#PRelu">PRelu</a>
* <a href="#Pad">Pad</a>
* <a href="#Pow">Pow</a>
* <a href="#RandomNormal">RandomNormal</a>
* <a href="#RandomNormalLike">RandomNormalLike</a>
* <a href="#RandomUniform">RandomUniform</a>
* <a href="#RandomUniformLike">RandomUniformLike</a>
* <a href="#Reciprocal">Reciprocal</a>
* <a href="#ReduceLogSumExp">ReduceLogSumExp</a>
* <a href="#ReduceMax">ReduceMax</a>
* <a href="#ReduceMean">ReduceMean</a>
* <a href="#ReduceMin">ReduceMin</a>
* <a href="#ReduceProd">ReduceProd</a>
* <a href="#ReduceSum">ReduceSum</a>
* <a href="#Relu">Relu</a>
* <a href="#Reshape">Reshape</a>
* <a href="#Selu">Selu</a>
* <a href="#Sigmoid">Sigmoid</a>
* <a href="#Slice">Slice</a>
* <a href="#Softmax">Softmax</a>
* <a href="#Split">Split</a>
* <a href="#Sqrt">Sqrt</a>
* <a href="#Squeeze">Squeeze</a>
* <a href="#Sub">Sub</a>
* <a href="#Sum">Sum</a>
* <a href="#Tanh">Tanh</a>
* <a href="#Transpose">Transpose</a>
* <a href="#ATen"><sub>experimental</sub> ATen</a>
* <a href="#Caffe2ConvTranspose"><sub>experimental</sub> Caffe2ConvTranspose</a>
* <a href="#ConstantFill"><sub>experimental</sub> ConstantFill</a>
* <a href="#FC"><sub>experimental</sub> FC</a>
* <a href="#GRUUnit"><sub>experimental</sub> GRUUnit</a>
* <a href="#GivenTensorFill"><sub>experimental</sub> GivenTensorFill</a>
* <a href="#Normalize"><sub>experimental</sub> Normalize</a>
* <a href="#Scale"><sub>experimental</sub> Scale</a>
* <a href="#SpatialBN"><sub>experimental</sub> SpatialBN</a>

### <a name="Abs"></a><a name="abs">**Abs**</a>

  Absolute takes one input data (Tensor<T>) and produces one output data
  (Tensor<T>) where the absolute is, y = abs(x), is applied to
  the tensor elementwise.

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd>Input tensor</dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd>Output tensor</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output types to float tensors.</dd>
</dl>


### <a name="Add"></a><a name="add">**Add**</a>

  Performs element-wise binary addition (with limited broadcast support).
  
  If necessary the right-hand-side argument will be broadcasted to match the
  shape of left-hand-side argument. When broadcasting is specified, the second
  tensor can either be of size 1 (a scalar value), or having its shape as a
  contiguous subset of the first tensor's shape. The starting of the mutually
  equal shape is specified by the argument "axis", and if it is not set, suffix
  matching is assumed. 1-dim expansion doesn't work yet.
  
  For example, the following tensor shapes are supported (with broadcast=1):
  
    shape(A) = (2, 3, 4, 5), shape(B) = (,), i.e. B is a scalar
    shape(A) = (2, 3, 4, 5), shape(B) = (5,)
    shape(A) = (2, 3, 4, 5), shape(B) = (4, 5)
    shape(A) = (2, 3, 4, 5), shape(B) = (3, 4), with axis=1
    shape(A) = (2, 3, 4, 5), shape(B) = (2), with axis=0
  
  Attribute `broadcast=1` needs to be passed to enable broadcasting.

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd>If set, defines the broadcast dimensions. See doc for details.</dd>
<dt><tt>broadcast</tt> : int</dt>
<dd>Pass 1 to enable broadcasting</dd>
</dl>

#### Inputs

<dl>
<dt><tt>A</tt> : T</dt>
<dd>First operand, should share the type with the second operand.</dd>
<dt><tt>B</tt> : T</dt>
<dd>Second operand. With broadcasting can be of smaller size than A. If broadcasting is disabled it should be of the same size.</dd>
</dl>

#### Outputs

<dl>
<dt><tt>C</tt> : T</dt>
<dd>Result, has same dimensions and type as A</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output types to float tensors.</dd>
</dl>


### <a name="ArgMax"></a><a name="argmax">**ArgMax**</a>

  Computes the indices of the max elements of the input tensor's element along the 
  provided axis. The resulted tensor has the same rank as the input if keepdims equal 1. 
  If keepdims equal 0, then the resulted tensor have the reduced dimension pruned. 
  The type of the output tensor is integer.

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd>The axis in which to compute the arg indices</dd>
<dt><tt>keepdims</tt> : int</dt>
<dd>Keep the reduced dimension or not, default 1 mean keep reduced dimension.</dd>
</dl>

#### Inputs

<dl>
<dt><tt>data</tt> : T</dt>
<dd>An input tensor.</dd>
</dl>

#### Outputs

<dl>
<dt><tt>reduced</tt> : T</dt>
<dd>Reduced output tensor with integer data type.</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output types to float tensors.</dd>
</dl>


### <a name="ArgMin"></a><a name="argmin">**ArgMin**</a>

  Computes the indices of the min elements of the input tensor's element along the 
  provided axis. The resulted tensor has the same rank as the input if keepdims equal 1. 
  If keepdims equal 0, then the resulted tensor have the reduced dimension pruned. 
  The type of the output tensor is integer.

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd>The axis in which to compute the arg indices</dd>
<dt><tt>keepdims</tt> : int</dt>
<dd>Keep the reduced dimension or not, default 1 mean keep reduced dimension.</dd>
</dl>

#### Inputs

<dl>
<dt><tt>data</tt> : T</dt>
<dd>An input tensor.</dd>
</dl>

#### Outputs

<dl>
<dt><tt>reduced</tt> : T</dt>
<dd>Reduced output tensor with integer data type.</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output types to float tensors.</dd>
</dl>


### <a name="AveragePool"></a><a name="averagepool">**AveragePool**</a>

  AveragePool consumes an input tensor X and applies average pooling across the
   the tensor according to kernel sizes, stride sizes, and pad lengths.
   Average pooling consisting of averaging all values of a subset of the
   input tensor according to the kernel size and downsampling the
   data into the output tensor Y for further processing.

#### Attributes

<dl>
<dt><tt>auto_pad</tt> : string</dt>
<dd>auto_pad must be either SAME_UPPER, SAME_LOWER or VALID. Where SAME_UPPER or SAME_LOWER mean pad the input so that the ouput size match the input.In case of odd number add the extra padding at the end for SAME_UPPER and at the begining for SAME_LOWER. VALID mean no padding, therefore, read the pixel values from the pads attribute.</dd>
<dt><tt>kernel_shape</tt> : list of ints</dt>
<dd>The size of the kernel along each axis.</dd>
<dt><tt>pads</tt> : list of ints</dt>
<dd>Padding for lower and upper side along each axis, it can take any value greater than or equal to 0. The value represent the number of pixels added to the lower and upper part of the corresponding axis. So `pads` will have two values per axis, first value corresponding to the number of pixels added to the begining of the axis and the second value corresponding to the number of pixels add at the end of the axis.</dd>
<dt><tt>strides</tt> : list of ints</dt>
<dd>Stride along each axis.</dd>
</dl>

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd>Input data tensor from the previous operator; dimensions for image case are (N x C x H x W), where N is the batch size, C is the number of channels, and H and W are the height and the width of the data. For non image case, the dimension are in the form of (N x C x D1 x D2 ... Dn), where N is the batch size.</dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd>Output data tensor from average pooling across the input tensor. Dimensions will vary based on various kernel, stride, and pad sizes.</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output types to float tensors.</dd>
</dl>


### <a name="BatchNormalization"></a><a name="batchnormalization">**BatchNormalization**</a>

  Carries out batch normalization as described in the paper
  https://arxiv.org/abs/1502.03167. Depending on the mode it is being run,
  there are multiple cases for the number of outputs, which we list below:
  
  Output case #1: Y, mean, var, saved_mean, saved_var (training mode)
  Output case #2: Y (test mode)

#### Attributes

<dl>
<dt><tt>epsilon</tt> : float</dt>
<dd>The epsilon value to use to avoid division by zero.</dd>
<dt><tt>is_test</tt> : int</dt>
<dd>If set to nonzero, run spatial batch normalization in test mode.</dd>
<dt><tt>momentum</tt> : float</dt>
<dd>Factor used in computing the running mean and variance.e.g., running_mean = running_mean * momentum + mean * (1 - momentum)</dd>
<dt><tt>spatial</tt> : int</dt>
<dd>Compute the mean and variance across all spatial elements or per feature.</dd>
</dl>

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd>The input 4-dimensional tensor of shape NCHW or NHWC depending on the order parameter.</dd>
<dt><tt>scale</tt> : T</dt>
<dd>The scale as a 1-dimensional tensor of size C to be applied to the output.</dd>
<dt><tt>bias</tt> : T</dt>
<dd>The bias as a 1-dimensional tensor of size C to be applied to the output.</dd>
<dt><tt>mean</tt> : T</dt>
<dd>The running mean (training) or the estimated mean (testing) as a 1-dimensional tensor of size C.</dd>
<dt><tt>var</tt> : T</dt>
<dd>The running variance (training) or the estimated variance (testing) as a 1-dimensional tensor of size C.</dd>
</dl>

#### Outputs (0 - &#8734;)

<dl>
<dt><tt>Y</tt> : T</dt>
<dd>The output 4-dimensional tensor of the same shape as X.</dd>
<dt><tt>mean</tt> : T</dt>
<dd>The running mean after the BatchNormalization operator. Must be in-place with the input mean. Should not be used for testing.</dd>
<dt><tt>var</tt> : T</dt>
<dd>The running variance after the BatchNormalization operator. Must be in-place with the input var. Should not be used for testing.</dd>
<dt><tt>saved_mean</tt> : T</dt>
<dd>Saved mean used during training to speed up gradient computation. Should not be used for testing.</dd>
<dt><tt>saved_var</tt> : T</dt>
<dd>Saved variance used during training to speed up gradient computation. Should not be used for testing.</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output types to float tensors.</dd>
</dl>


### <a name="Cast"></a><a name="cast">**Cast**</a>

  The operator casts the elements of a given input tensor to a data type
  specified by the 'to' argument and returns an output tensor of the same size in
  the converted type. The 'to' argument must be one of the data types specified
  in the 'DataType' enum field in the TensorProto message. If the 'to' argument
  is not provided or is not one of the enumerated types in DataType, Caffe2
  throws an Enforce error.
  
  NOTE: Casting to and from strings is not supported yet.

#### Attributes

<dl>
<dt><tt>to</tt> : string</dt>
<dd>The data type to which the elements of the input tensor are cast.Strictly must be one of the types from DataType enum in TensorProto</dd>
</dl>

#### Inputs

<dl>
<dt><tt>input</tt> : T1</dt>
<dd>Input tensor to be cast.</dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T2</dt>
<dd>Output tensor with the same shape as input with type specified by the 'to' argument</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T1</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input types to float tensors.</dd>
<dt><tt>T2</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain output types to float tensors.</dd>
</dl>


### <a name="Ceil"></a><a name="ceil">**Ceil**</a>

  Ceil takes one input data (Tensor<T>) and produces one output data
  (Tensor<T>) where the ceil is, y = ceil(x), is applied to
  the tensor elementwise.

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd>Input tensor</dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd>Output tensor</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output types to float tensors.</dd>
</dl>


### <a name="Concat"></a><a name="concat">**Concat**</a>

  Concatenate a list of tensors into a single tensor

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd>Which axis to concat on</dd>
</dl>

#### Inputs (1 - &#8734;)

<dl>
<dt><tt>inputs...</tt> : T</dt>
<dd>List of tensors for concatenation</dd>
</dl>

#### Outputs

<dl>
<dt><tt>concat_result</tt> : T</dt>
<dd>Concatenated tensor</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain output types to float tensors.</dd>
</dl>


### <a name="Constant"></a><a name="constant">**Constant**</a>

  A constant tensor.

#### Attributes

<dl>
<dt><tt>value</tt> : tensor</dt>
<dd>The value for the elements of the output tensor.</dd>
</dl>

#### Inputs


#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd>Output tensor containing the same value of the provided tensor.</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output types to float tensors.</dd>
</dl>


### <a name="Conv"></a><a name="conv">**Conv**</a>

  The convolution operator consumes an input tensor and a filter, and
  computes the output.

#### Attributes

<dl>
<dt><tt>auto_pad</tt> : string</dt>
<dd>auto_pad must be either SAME_UPPER, SAME_LOWER or VALID. Where SAME_UPPER or SAME_LOWER mean pad the input so that the ouput size match the input.In case of odd number add the extra padding at the end for SAME_UPPER and at the begining for SAME_LOWER. VALID mean no padding, therefore, read the pixel values from the pads attribute.</dd>
<dt><tt>dilations</tt> : list of ints</dt>
<dd>dilation value along each axis of the filter.</dd>
<dt><tt>group</tt> : int</dt>
<dd>number of groups input channels and output channels are divided into</dd>
<dt><tt>kernel_shape</tt> : list of ints</dt>
<dd>The shape of the convolution kernel.</dd>
<dt><tt>pads</tt> : list of ints</dt>
<dd>Padding for lower and upper side along each axis, it can take any value greater than or equal to 0. The value represent the number of pixels added to the lower and upper part of the corresponding axis. So `pads` will have two values per axis, first value corresponding to the number of pixels added to the begining of the axis and the second value corresponding to the number of pixels add at the end of the axis.</dd>
<dt><tt>strides</tt> : list of ints</dt>
<dd>stride along each axis.</dd>
</dl>

#### Inputs (2 - 3)

<dl>
<dt><tt>X</tt> : T</dt>
<dd>Input data tensor from previous layer; has size (N x C x H x W), where N is the batch size, C is the number of channels, and H and W are the height and width. Note that this is for the 2D image.Otherwise the size is (N x D1 x D2 ... x Dn)</dd>
<dt><tt>weights</tt> : T</dt>
<dd>The weight tensor that will be used in the convolutions; has size (M x C x kH x kW), where C is the number of channels, and kH and kW are the height and width of the kernel, and M is the number of feature maps. For more than 2 dimensions, the kernel shape will be (M x C x k1 x k2 x ... x kn), where is the dimension of the kernel</dd>
<dt><tt>bias</tt> : T</dt>
<dd>Optional 1D bias to be added to the convolution, has size of M.</dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd>Output data tensor that contains the result of the convolution. The output dimensions are functions of the kernel size, stride size, and pad lengths.</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output types to float tensors.</dd>
</dl>


### <a name="ConvTranspose"></a><a name="convtranspose">**ConvTranspose**</a>

  The convolution transpose operator consumes an input tensor and a filter,
  and computes the output.

#### Attributes

<dl>
<dt><tt>auto_pad</tt> : string</dt>
<dd>auto_pad must be either SAME_UPPER, SAME_LOWER or VALID. Where SAME_UPPER or SAME_LOWER mean pad the input so that the ouput size match the input.In case of odd number add the extra padding at the end for SAME_UPPER and at the begining for SAME_LOWER. VALID mean no padding, therefore, read the pixel values from the pads attribute.</dd>
<dt><tt>dilations</tt> : list of ints</dt>
<dd>dilation value along each axis of the filter.</dd>
<dt><tt>group</tt> : int</dt>
<dd>number of groups input channels and output channels are divided into</dd>
<dt><tt>kernel_shape</tt> : list of ints</dt>
<dd>The shape of the convolution kernel.</dd>
<dt><tt>output_shape</tt> : list of ints</dt>
<dd>The shape of the output.</dd>
<dt><tt>pads</tt> : list of ints</dt>
<dd>Padding for lower and upper side along each axis, it can take any value greater than or equal to 0. The value represent the number of pixels added to the lower and upper part of the corresponding axis. So `pads` will have two values per axis, first value corresponding to the number of pixels added to the begining of the axis and the second value corresponding to the number of pixels add at the end of the axis.</dd>
<dt><tt>strides</tt> : list of ints</dt>
<dd>stride along each axis.</dd>
</dl>

#### Inputs (2 - 3)

<dl>
<dt><tt>X</tt> : T</dt>
<dd>Input data tensor from previous layer; has size (N x C x H x W), where N is the batch size, C is the number of channels, and H and W are the height and width. Note that this is for the 2D image.Otherwise the size is (N x D1 x D2 ... x Dn)</dd>
<dt><tt>weights</tt> : T</dt>
<dd>The weight tensor that will be used in the convolutions; has size (C x M x kH x kW), where C is the number of channels, and kH and kW are the height and width of the kernel, and M is the number of feature maps. For more than 2 dimensions, the kernel shape will be (C x M x k1 x k2 x ... x kn), where is the dimension of the kernel</dd>
<dt><tt>bias</tt> : T</dt>
<dd>Optional 1D bias to be added to the convolution, has size of C.</dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd>Output data tensor that contains the result of the convolution. The output dimensions are functions of the kernel size, stride size, and pad lengths.</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output types to float tensors.</dd>
</dl>


### <a name="Div"></a><a name="div">**Div**</a>

  Performs element-wise binary division (with limited broadcast support).
  
  If necessary the right-hand-side argument will be broadcasted to match the
  shape of left-hand-side argument. When broadcasting is specified, the second
  tensor can either be of size 1 (a scalar value), or having its shape as a
  contiguous subset of the first tensor's shape. The starting of the mutually
  equal shape is specified by the argument "axis", and if it is not set, suffix
  matching is assumed. 1-dim expansion doesn't work yet.
  
  For example, the following tensor shapes are supported (with broadcast=1):
  
    shape(A) = (2, 3, 4, 5), shape(B) = (,), i.e. B is a scalar
    shape(A) = (2, 3, 4, 5), shape(B) = (5,)
    shape(A) = (2, 3, 4, 5), shape(B) = (4, 5)
    shape(A) = (2, 3, 4, 5), shape(B) = (3, 4), with axis=1
    shape(A) = (2, 3, 4, 5), shape(B) = (2), with axis=0
  
  Attribute `broadcast=1` needs to be passed to enable broadcasting.

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd>If set, defines the broadcast dimensions. See doc for details.</dd>
<dt><tt>broadcast</tt> : int</dt>
<dd>Pass 1 to enable broadcasting</dd>
</dl>

#### Inputs

<dl>
<dt><tt>A</tt> : T</dt>
<dd>First operand, should share the type with the second operand.</dd>
<dt><tt>B</tt> : T</dt>
<dd>Second operand. With broadcasting can be of smaller size than A. If broadcasting is disabled it should be of the same size.</dd>
</dl>

#### Outputs

<dl>
<dt><tt>C</tt> : T</dt>
<dd>Result, has same dimensions and type as A</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output types to float tensors.</dd>
</dl>


### <a name="Dropout"></a><a name="dropout">**Dropout**</a>

  Dropout takes one input data (Tensor<float>) and produces two Tensor outputs,
  output (Tensor<float>) and mask (Tensor<bool>). Depending on whether it is in
  test mode or not, the output Y will either be a random dropout, or a simple
  copy of the input. Note that our implementation of Dropout does scaling in
  the training phase, so during testing nothing needs to be done.

#### Attributes

<dl>
<dt><tt>is_test</tt> : int</dt>
<dd>(int, default 0) if nonzero, run dropout in test mode where the output is simply Y = X.</dd>
<dt><tt>ratio</tt> : float</dt>
<dd>(float, default 0.5) the ratio of random dropout</dd>
</dl>

#### Inputs

<dl>
<dt><tt>data</tt> : T</dt>
<dd>The input data as Tensor.</dd>
</dl>

#### Outputs (1 - 2)

<dl>
<dt><tt>output</tt> : T</dt>
<dd>The output.</dd>
<dt><tt>mask</tt> : T</dt>
<dd>The output mask. If is_test is nonzero, this output is not filled.</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output types to float tensors.</dd>
</dl>


### <a name="Elu"></a><a name="elu">**Elu**</a>

  Elu takes one input data (Tensor<T>) and produces one output data
  (Tensor<T>) where the function `f(x) = alpha * (exp(x) - 1.) for x <
  0`, `f(x) = x for x >= 0`., is applied to the tensor elementwise.
  

#### Attributes

<dl>
<dt><tt>alpha</tt> : float</dt>
<dd>Coefficient of ELU default to 1.0.</dd>
</dl>

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd>1D input tensor</dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd>1D input tensor</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output types to float tensors.</dd>
</dl>


### <a name="Exp"></a><a name="exp">**Exp**</a>

  Calculates the exponential of the given input tensor, element-wise. This
  operation can be done in an in-place fashion too, by providing the same input
  and output blobs.

#### Inputs

<dl>
<dt><tt>input</tt> : T</dt>
<dd>Input tensor</dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd>The exponential of the input tensor computed element-wise</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output types to float tensors.</dd>
</dl>


### <a name="Flatten"></a><a name="flatten">**Flatten**</a>

  Flattens the input tensor into a 2D matrix. If input tensor has shape
  (d_0, d_1, ... d_n) then the output will have shape
  (d_0 X d_1 ... d_(axis-1), d_axis X d_(axis+1) ... X dn).

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd>(Default to 1) Indicate up to which input dimensions (exclusive) should be flattened to the outer dimension of the output</dd>
</dl>

#### Inputs

<dl>
<dt><tt>input</tt> : T</dt>
<dd>A tensor of rank >= axis.</dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd>A 2D tensor with the contents of the input tensor, with input dimensions up to axis flattened to the outer dimension of the output and remaining input dimensions flattened into the inner dimension of the output.</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output types to float tensors.</dd>
</dl>


### <a name="Floor"></a><a name="floor">**Floor**</a>

  Floor takes one input data (Tensor<T>) and produces one output data
  (Tensor<T>) where the floor is, y = floor(x), is applied to
  the tensor elementwise.

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd>Input tensor</dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd>Output tensor</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output types to float tensors.</dd>
</dl>


### <a name="Gather"></a><a name="gather">**Gather**</a>

  Given DATA tensor of rank r >= 1, and INDICES tensor of rank q, gather
  entries of the outer-most dimension of DATA indexed by INDICES, and concatenate
  them in an output tensor of rank q + (r - 1).
  
  Example:
    DATA  = [
        [1.0, 1.2],
        [2.3, 3.4],
        [4.5, 5.7],
    ]
    INDICES = [
        [0, 1],
        [1, 2],
    ]
    OUTPUT = [
        [
            [1.0, 1.2],
            [2.3, 3.4],
        ],
        [
            [2.3, 3.4],
            [4.5, 5.7],
        ],
    ]

#### Inputs

<dl>
<dt><tt>DATA</tt> : T</dt>
<dd>Tensor of rank r >= 1.</dd>
<dt><tt>INDICES</tt> : T</dt>
<dd>Tensor of int32/int64 indices, of any rank q.</dd>
</dl>

#### Outputs

<dl>
<dt><tt>OUTPUT</tt> : T</dt>
<dd>Tensor of rank q + (r - 1).</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output types to float tensors.</dd>
</dl>


### <a name="Gemm"></a><a name="gemm">**Gemm**</a>

  General Matrix multiplication:
  https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms#Level_3
  Compute Y = alpha * A * B + beta * C, where input tensor A has dimension (M X K)
  , input tensor B has dimension (K X N), input tensor C and output tensor Y have
  dimension (M X N). Input tensor C can be used inplace as the output tensor Y.
  If attribute broadcast is non-zero, input tensor C will be broadcasted to match
  the dimension requirement. If A can be transposed before doing the computation
  if attribute transA is non-zero, same for B and transB.

#### Attributes

<dl>
<dt><tt>alpha</tt> : float</dt>
<dd>Scalar multiplier for the product of input tensors A * B</dd>
<dt><tt>beta</tt> : float</dt>
<dd>Scalar multiplier for input tensor C</dd>
<dt><tt>broadcast</tt> : int</dt>
<dd>Whether C should be broadcasted</dd>
<dt><tt>transA</tt> : int</dt>
<dd>Whether A should be transposed</dd>
<dt><tt>transB</tt> : int</dt>
<dd>Whether B should be transposed</dd>
</dl>

#### Inputs

<dl>
<dt><tt>A</tt> : T</dt>
<dd>Input tensor A</dd>
<dt><tt>B</tt> : T</dt>
<dd>Input tensor B</dd>
<dt><tt>C</tt> : T</dt>
<dd>Input tensor C, can be inplace.</dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd>Output tensor.</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output types to float tensors.</dd>
</dl>


### <a name="GlobalAveragePool"></a><a name="globalaveragepool">**GlobalAveragePool**</a>

  GlobalAveragePool consumes an input tensor X and applies average pooling across the
   the values in the same channel. This is equivalent to AveragePool with kernel size
   equal to the spatial dimension of input tensor.

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd>Input data tensor from the previous operator; dimensions for image case are (N x C x H x W), where N is the batch size, C is the number of channels, and H and W are the height and the width of the data. For non image case, the dimension are in the form of (N x C x D1 x D2 ... Dn), where N is the batch size.</dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd>Output data tensor from pooling across the input tensor. Dimensions will be N x C x 1 x 1</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output types to float tensors.</dd>
</dl>


### <a name="GlobalLpPool"></a><a name="globallppool">**GlobalLpPool**</a>

  GlobalLpPool consumes an input tensor X and applies lp pool pooling across the
   the values in the same channel. This is equivalent to LpPool with kernel size
   equal to the spatial dimension of input tensor.

#### Attributes

<dl>
<dt><tt>p</tt> : float</dt>
<dd>p value of the Lp norm used to pool over the input data, default is 2.0.</dd>
</dl>

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd>Input data tensor from the previous operator; dimensions for image case are (N x C x H x W), where N is the batch size, C is the number of channels, and H and W are the height and the width of the data. For non image case, the dimension are in the form of (N x C x D1 x D2 ... Dn), where N is the batch size.</dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd>Output data tensor from pooling across the input tensor. Dimensions will be N x C x 1 x 1</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output types to float tensors.</dd>
</dl>


### <a name="GlobalMaxPool"></a><a name="globalmaxpool">**GlobalMaxPool**</a>

  GlobalMaxPool consumes an input tensor X and applies max pooling across the
   the values in the same channel. This is equivalent to MaxPool with kernel size
   equal to the spatial dimension of input tensor.

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd>Input data tensor from the previous operator; dimensions for image case are (N x C x H x W), where N is the batch size, C is the number of channels, and H and W are the height and the width of the data. For non image case, the dimension are in the form of (N x C x D1 x D2 ... Dn), where N is the batch size.</dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd>Output data tensor from pooling across the input tensor. Dimensions will be N x C x 1 x 1</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output types to float tensors.</dd>
</dl>


### <a name="LRN"></a><a name="lrn">**LRN**</a>

  Local Response Normalization. It normalizes over local input regions.
  Each input value is divided by
  (bias+(alpha/size)*sum(xi^2 for every xi in the local region))^beta.

#### Attributes

<dl>
<dt><tt>alpha</tt> : float (required)</dt>
<dd>Scaling parameter</dd>
<dt><tt>beta</tt> : float (required)</dt>
<dd>The exponent</dd>
<dt><tt>bias</tt> : float</dt>
<dd>Default to 1</dd>
<dt><tt>size</tt> : int (required)</dt>
<dd>The number of channels to sum over</dd>
</dl>

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd>Input tensor</dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd>Output tensor</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output  types to float tensors.</dd>
</dl>


### <a name="LeakyRelu"></a><a name="leakyrelu">**LeakyRelu**</a>

  LeakyRelu takes input data (Tensor<T>) and an argument alpha, and produces one
  output data (Tensor<T>) where the function `f(x) = alpha * x for x < 0`,
  `f(x) = x for x >= 0`, is applied to the data tensor elementwise.

#### Attributes

<dl>
<dt><tt>alpha</tt> : float</dt>
<dd>Coefficient of leakage</dd>
</dl>

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd>Input tensor</dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd>Output tensor</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output types to float tensors.</dd>
</dl>


### <a name="Log"></a><a name="log">**Log**</a>

  Calculates the natural log of the given input tensor, element-wise. This
  operation can be done in an in-place fashion too, by providing the same input
  and output blobs.

#### Inputs

<dl>
<dt><tt>input</tt> : T</dt>
<dd>Input tensor</dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd>The natural log of the input tensor computed element-wise</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output types to float tensors.</dd>
</dl>


### <a name="LpPool"></a><a name="lppool">**LpPool**</a>

  LpPool consumes an input tensor X and applies Lp pooling across the
   the tensor according to kernel sizes, stride sizes, and pad lengths.
   Lp pooling consisting of computing the Lp norm on all values of a subset 
   of the input tensor according to the kernel size and downsampling the
   data into the output tensor Y for further processing.

#### Attributes

<dl>
<dt><tt>auto_pad</tt> : string</dt>
<dd>auto_pad must be either SAME_UPPER, SAME_LOWER or VALID. Where SAME_UPPER or SAME_LOWER mean pad the input so that the ouput size match the input.In case of odd number add the extra padding at the end for SAME_UPPER and at the begining for SAME_LOWER. VALID mean no padding, therefore, read the pixel values from the pads attribute.</dd>
<dt><tt>dilations</tt> : list of ints</dt>
<dd>Dilation along each axis, 1 means no dilation.</dd>
<dt><tt>kernel_shape</tt> : list of ints</dt>
<dd>The size of the kernel along each axis.</dd>
<dt><tt>p</tt> : float</dt>
<dd>p value of the Lp norm used to pool over the input data, default is 2.0.</dd>
<dt><tt>pads</tt> : list of ints</dt>
<dd>Padding for lower and upper side along each axis, it can take any value greater than or equal to 0. The value represent the number of pixels added to the lower and upper part of the corresponding axis. So `pads` will have two values per axis, first value corresponding to the number of pixels added to the begining of the axis and the second value corresponding to the number of pixels add at the end of the axis.</dd>
<dt><tt>strides</tt> : list of ints</dt>
<dd>Stride along each axis.</dd>
</dl>

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd>Input data tensor from the previous operator; dimensions for image case are (N x C x H x W), where N is the batch size, C is the number of channels, and H and W are the height and the width of the data. For non image case, the dimension are in the form of (N x C x D1 x D2 ... Dn), where N is the batch size.</dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd>Output data tensor from Lp pooling across the input tensor. Dimensions will vary based on various kernel, stride, and pad sizes.</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output types to float tensors.</dd>
</dl>


### <a name="MatMul"></a><a name="matmul">**MatMul**</a>

  Matrix product that behaves like numpy.matmul: https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.matmul.html

#### Inputs

<dl>
<dt><tt>A</tt> : T</dt>
<dd>N-dimensional matrix A</dd>
<dt><tt>B</tt> : T</dt>
<dd>N-dimensional matrix B</dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd>Matrix multiply results from A * B</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output types to float tensors.</dd>
</dl>


### <a name="Max"></a><a name="max">**Max**</a>

  Element-wise max of each of the input tensors. The first input tensor can be
  used in-place as the output tensor, in which case the max will be done in
  place and results will be accumulated in input0. All inputs and outputs must
  have the same shape and data type.

#### Inputs (1 - &#8734;)

<dl>
<dt><tt>data_0</tt> : T</dt>
<dd>First of the input tensors. Can be inplace.</dd>
</dl>

#### Outputs

<dl>
<dt><tt>max</tt> : T</dt>
<dd>Output tensor. Same dimension as inputs.</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output types to float tensors.</dd>
</dl>


### <a name="MaxPool"></a><a name="maxpool">**MaxPool**</a>

  MaxPool consumes an input tensor X and applies max pooling across the
   the tensor according to kernel sizes, stride sizes, and pad lengths.
   Max pooling consisting of getting the max value of a subset of the
   input tensor according to the kernel size and downsampling the
   data into the output tensor Y for further processing.

#### Attributes

<dl>
<dt><tt>auto_pad</tt> : string</dt>
<dd>auto_pad must be either SAME_UPPER, SAME_LOWER or VALID. Where SAME_UPPER or SAME_LOWER mean pad the input so that the ouput size match the input.In case of odd number add the extra padding at the end for SAME_UPPER and at the begining for SAME_LOWER. VALID mean no padding, therefore, read the pixel values from the pads attribute.</dd>
<dt><tt>dilations</tt> : list of ints</dt>
<dd>Dilation along each axis, 1 means no dilation.</dd>
<dt><tt>kernel_shape</tt> : list of ints</dt>
<dd>The size of the kernel along each axis.</dd>
<dt><tt>pads</tt> : list of ints</dt>
<dd>Padding for lower and upper side along each axis, it can take any value greater than or equal to 0. The value represent the number of pixels added to the lower and upper part of the corresponding axis. So `pads` will have two values per axis, first value corresponding to the number of pixels added to the begining of the axis and the second value corresponding to the number of pixels add at the end of the axis.</dd>
<dt><tt>strides</tt> : list of ints</dt>
<dd>Stride along each axis.</dd>
</dl>

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd>Input data tensor from the previous operator; dimensions for image case are (N x C x H x W), where N is the batch size, C is the number of channels, and H and W are the height and the width of the data. For non image case, the dimension are in the form of (N x C x D1 x D2 ... Dn), where N is the batch size.</dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd>Output data tensor from max pooling across the input tensor. Dimensions will vary based on various kernel, stride, and pad sizes.</dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd>Constrain input and output types to float tensors.</dd>
</dl>


